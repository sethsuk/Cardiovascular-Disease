# -*- coding: utf-8 -*-
"""FINAL 5450 Project - Cardiovascular Disease - Eric, Grace, Seth

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kusVP2-lf8YSQir42bIEVLZ4gAtopZSO

# Cardiovascular Disease
*Eric Sun, Seth Sukboontip, Grace Deng*

# Part 1: Introduction

## Overview

[Rubric](https://docs.google.com/document/d/1d1lMMpLAFZGnHY8Y78r4TSkSBsDmSVQ6JzTds5nxrGc/edit?tab=t.0)

[Proposal](https://docs.google.com/document/d/1wvYbaLU5f1bPXRKHRsVHHFtYjFDW5ZPrRn6CvTWKFJo/edit?tab=t.npzgzqx2m60v#heading=h.16wsfhwb7n99)

[Details](https://docs.google.com/document/d/1czdw5IbTneU7l5tXSmxTi3gFsSYFtCZoKAgA3AcHRu8/edit?tab=t.0#heading=h.2et92p0)

[Example](https://colab.research.google.com/drive/11y8gOg8Ji-JAu4oKlcEaQxkySdDAGnJH?usp=sharing#scrollTo=r-Zw-o05Dat8)

## Our Data

This notebook uses a subset of the Behavioral Risk Factor Surveillance System (BRFSS) dataset, a large-scale annual health survey conducted by the CDC. The BRFSS collects self-reported information from U.S. adults on health-related risk behaviors, chronic health conditions, and use of preventive services, making it one of the most comprehensive sources of public health data in the United States.

To focus the analysis on cardiovascular disease (CVD) risk, the original dataset—with over 300 variables—was carefully preprocessed and filtered. From Kaggle use Alphiree selected 19 lifestyle-related variables were selected based on their relevance to behavioral and health risk factors known to be associated with CVD (e.g., smoking, alcohol use, physical activity, diet, and general health status). This cleaned and focused dataset serves as the foundation for building and evaluating predictive models that aim to classify individuals at risk for cardiovascular disease based on their reported habits and health conditions.

A link to the dataset we used can be found [here](https://www.kaggle.com/datasets/alphiree/cardiovascular-diseases-risk-prediction-dataset/data).

One major issue in our dataset is that there is a high imbalance in the heart disease column, which we are trying to predict. As we can see from the logistic regression baseline model. We have a high accuracy but a very poor recall score, with those with heart disease. Therefore, we would like to explore the possibility of upsampling our heart disease data to help resolve the class imbalance.

We also initially, had some issues with the types of data provided. Some of the data contains strings that are not easily encoded. We used several techniques. First, with the General_Health and Checkup categories, because although they were categorical, there was an ordinal relationship between the values. However for categories like Diabetes, where there were 4 different options with no inherent order, we used one hot encoding.

# Part 2: Data Loading & Preprocessing

## Loading Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tqdm.notebook as tqdm

from google.colab import drive
drive.mount('/content/drive')

"""It is necessary to download the csv from [Kaggle](https://www.google.com/url?q=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Falphiree%2Fcardiovascular-diseases-risk-prediction-dataset%2Fdata) and upload it to your Google Drive's main folder (MyDrive)."""

cvd_df = pd.read_csv('/content/drive/MyDrive/CVD_cleaned.csv')

cvd_df.head()

"""## Analyzing Data Structure

### Numerical Data

We can start by understanding the numerical data in our dataset. Some columns like `Height`, `Weight`, and `BMI` have standard units.

For the other columns related to behavior and consumption, we referenced the original BRFSS survey questions and our dataset source to interpret their meanings:

* `Alcohol_Consumption`:
Number of **days** in the past 30 days the respondent consumed at least one alcoholic drink.

* `Fruit_Consumption`:
Number of **times** fruit was eaten in the past 30 days.

* `Green_Vegetables_Consumption`:
Number of **times** dark green vegetables (e.g., spinach, broccoli) were eaten in the past 30 days.

* `FriedPotato_Consumption`:
Number of **times** fried potatoes (e.g., French fries, hash browns) were eaten in the past 30 days.
"""

cvd_df.describe()

"""#### Takeaways at a Glance

* **BMI**: The average BMI is **28.6**, placing the typical respondent in the **overweight** category (BMI 25–29.9), with some values reaching well into the obese range. Note that the average BMI in the U.S. falls within this range, considering the average of **29.7** reported by the CDC's 2011–2018 National Health and Nutrition Surveys.

* **Alcohol Consumption**: The median respondent drank alcohol on **1 day** in the past month, though the average is **5.1 days**, indicating a **skewed distribution** with heavier drinkers pulling the mean upward. According to the CDC, about **16.6% of U.S. adults** reported binge drinking in the past 30 days, with prevalence highest among those aged 25–34 years.

* **Fruit Consumption**: The median number of times fruit was consumed in the past 30 days is **30**, or roughly **once per day**, aligning with the USDA's Dietary Guidelines for Americans 2020–2025, which recommend **1.5–2 cup-equivalents** of fruits daily for adults.

* **Green Vegetables Consumption**: Median intake is **12 times per month**, or about **3 times per week**, which may be lower than recommended. The USDA advises adults to consume **2–3 cup-equivalents** of vegetables daily, including a variety of types such as dark green vegetables.

* **Fried Potato Consumption**: Median frequency is **4 times per month**, but with a long tail—some individuals reported consuming fried potatoes more than **4 times per week**. Frequent consumption of fried foods has been linked to increased health risks. Notably, during 2013–2016, **36.6% of U.S. adults** consumed fast food on a given day, with higher consumption among younger adults.

### Categorical Data

Now, we can take a look at our categorical features.
"""

categorical_cols = cvd_df.dtypes[cvd_df.dtypes != 'float64'].index.tolist()
print(categorical_cols)

cvd_df[categorical_cols].head()

"""We can see that quite a few of the columns are binary, only containing Yes and No values. These can be converted to 0s and 1s in our data cleaning."""

yes_no_cols = []
for col in categorical_cols:
    unique_vals = cvd_df[col].unique()
    if all(val in ['Yes', 'No'] for val in unique_vals):
        yes_no_cols.append(col)
yes_no_cols

"""Let's look at the remaining columns, in order to better understand how we could encode them in our feature engineering. We see that these columns contain categorical values, some being ordinal and others being nominal."""

other_cols = [col for col in categorical_cols if col not in yes_no_cols]
other_cols

cvd_df['General_Health'].value_counts()

cvd_df['Checkup'].value_counts()

cvd_df['Diabetes'].value_counts()

cvd_df['Sex'].value_counts()

cvd_df['Age_Category'].value_counts()

"""# Part 3: Exploratory Data Analysis

## Target: Heart Disease
This pie chart displays the distribution of the target variable Heart_Disease, indicating whether an individual has been diagnosed with cardiovascular disease. The dataset is highly imbalanced, with 91.9% of individuals not reporting heart disease and only 8.1% indicating a diagnosis. This imbalance is important to consider during model development, as it can lead to biased predictions favoring the majority class unless appropriate techniques (class weighting in Part 8, resampling in Part 7) are applied.
"""

cvd_df['Heart_Disease'].value_counts().plot(kind='pie', autopct='%1.1f%%', title='Heart_Disease')

"""## Potential Comorbidities
Comorbid conditions such as Depression, Arthritis, and Diabetes are often associated with increased cardiovascular disease (CVD) risk and may serve as important behavioral and physiological predictors. These pie charts show their distribution across the dataset:

* Depression is reported by 20.0% of individuals.
* Arthritis is present in 32.7% of cases, making it the most common among the three.
* Diabetes affects 13.0%, with an additional 2.2% reporting pre-diabetes or borderline diabetes, and 0.9% indicating gestational diabetes.

These figures suggest a notable presence of chronic health conditions that may co-occur with or contribute to heart disease, warranting their inclusion as predictive features in our model.
"""

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Depression pie chart
cvd_df['Depression'].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=axes[0], title='Depression')

# Arthritis pie chart
cvd_df['Arthritis'].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=axes[1], title='Arthritis')

# Diabetes pie chart
cvd_df['Diabetes'].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=axes[2], title='Diabetes')

plt.show()

"""## Lifestyle Factors

As noted in our initial Data Structure Analysis (Part 2.2), these features represent frequency per month. Note that `Alcohol_Consumption` represents days per month, not frequency.

- **Fruit and Green Vegetable Consumption** show moderate median intake levels, with some individuals reporting high frequencies (>100 times/month). These behaviors are generally associated with protective effects against cardiovascular disease.

- **Fried Potato Consumption** is more heavily skewed toward low to moderate consumption, but also exhibits long tails, indicating a subset of individuals with high intake—a potential dietary risk factor.

- **Alcohol Consumption**, measured in days per month, is typically low for most respondents. However, a small number report near-daily drinking, which could increase CVD risk.

These variables reflect modifiable behaviors and will be important in understanding how lifestyle choices relate to heart disease outcomes.


"""

cols = [
    'Fruit_Consumption',
    'Green_Vegetables_Consumption',
    'FriedPotato_Consumption'
]

fig, axes = plt.subplots(1, 2, figsize=(15, 5))

cvd_df[cols].plot(kind='box', ax=axes[0], title='Lifestyle Factors (Times per Month)')
cvd_df['Alcohol_Consumption'].plot(kind='box', ax=axes[1], title='Alcohol Consumption (Days per Month)')

plt.tight_layout()
plt.show()

cvd_df['Exercise'].value_counts().plot(kind='pie', autopct='%1.1f%%', title='Exercise')

"""## Anthropometric Measures
- **Height** and **Weight** exhibit approximately normal distributions, with most individuals clustered around 160–180 cm in height and 60–100 kg in weight.

- **Body Mass Index (BMI)**, derived from height and weight, is skewed right, indicating a substantial portion of individuals fall above the healthy BMI range of 18.5–24.9 (shaded in green).

A large number of participants are classified as overweight or obese (BMI > 25), which is a well-established risk factor for cardiovascular disease.

These variables offer insight into overall body composition and may help uncover trends related to physical health and heart disease prevalence.

One note is that BMI is derived from Height and Weight, so it may have a high correlation with them. (Note in hindsight: See Part 4, Feature Engineering - Correlation Testing!)
"""

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Height histogram
cvd_df['Height_(cm)'].plot(kind='hist', ax=axes[0], title='Height (cm)', bins=20)

# Weight histogram
cvd_df['Weight_(kg)'].plot(kind='hist', ax=axes[1], title='Weight (kg)', bins=20)

# BMI histogram
cvd_df['BMI'].plot(kind='hist', ax=axes[2], title='BMI', bins=20)
axes[2].axvspan(18.5, 24.9, color='green', alpha=0.3, label='Healthy Range')
axes[2].legend()

plt.tight_layout()
plt.show()

"""# Part 4: Feature Engineering

## Data Cleaning

Next, we need to prepare the data for further analysis with our machine learning models. We will first create a copy so that we won't accidentally damage the original data.
"""

cleaned_cvd_df = cvd_df.copy()

"""For the other binary features, we simply mapped the yes values to a 1 and the no values to a 0, making it easier for our models to understand the binary differences."""

cat_binary = ['Exercise', 'Heart_Disease', 'Skin_Cancer', 'Other_Cancer', 'Depression', 'Arthritis', 'Smoking_History']

for cat in cat_binary:
  cleaned_cvd_df[cat] = cleaned_cvd_df[cat].replace({'Yes': 1, 'No': 0})

"""Next, we have our general health categories. Since it's ordered fron Poor to Excellent, we are able to employ an ordinal encoding method since each number carries meaning relative to its neighbors. That is, having a general_health of 4 is better 2, whic is better than 0."""

convert = {"Poor" : 0, 'Fair' : 1, 'Good' : 2, 'Very Good' : 3, "Excellent" : 4}
cleaned_cvd_df['General_Health'] = cleaned_cvd_df['General_Health'].map(convert)

"""One column to note is the `Diabetes` column since it not only contains the basic "Yes" and "No" values, but it also contains modified Yes and No. We were able to circumvent this issue by creating additional flags to show the nuanced types of diabetes."""

cleaned_cvd_df['Diabetes'].value_counts()

cleaned_cvd_df['Has_Diabetes'] = (cleaned_cvd_df['Diabetes'] == 'Yes').astype(int)

# Flag for pre-diabetes or borderline diabetes
cleaned_cvd_df['PreDiabetes_Flag'] = (cleaned_cvd_df['Diabetes'] == 'No, pre-diabetes or borderline diabetes').astype(int)

# Flag for gestational diabetes (female told only during pregnancy)
cleaned_cvd_df['Gestational_Female_Flag'] = (cleaned_cvd_df['Diabetes'] == 'Yes, but female told only during pregnancy').astype(int)

cleaned_cvd_df = cleaned_cvd_df.drop(columns=['Diabetes'])

"""We also converted the `Sex` column to the `Male` column with a simple 0, 1 flag."""

cleaned_cvd_df['Male'] = cleaned_cvd_df['Male'] = (cleaned_cvd_df['Sex'] == 'Male').astype(int)

cleaned_cvd_df = cleaned_cvd_df.drop(columns=['Sex'])

"""We also applied a similar ordinal encodng for the `Checkup` column. Although, note that we also have a `Had_Checkup` column to account for those who never had a checkup before, which should be weighted more."""

# Encode Checkup column
cleaned_cvd_df['Had_Checkup'] = cleaned_cvd_df['Checkup'].apply(lambda x: 0 if x == 'Never' else 1)
cleaned_cvd_df['Checkup'] = cleaned_cvd_df['Checkup'].replace({'5 or more years ago': 6, 'Within the past 5 years': 5, 'Within the past 2 years': 2, 'Within the past year': 1, 'Never': 7})

"""Lastly, we cleaned the `Age_Category` column simply extracting the range's lower bound and storing it as an integer for easier training."""

cleaned_cvd_df['Age_Category'].value_counts()

def extract_lower_bound(age_str):
    if '+' in age_str:
        return int(age_str.replace('+', ''))
    elif '-' in age_str:
        return int(age_str.split('-')[0])
    else:
        return None  # fallback in case of unexpected format

# Apply to the column
cleaned_cvd_df['Age_Lower'] = cleaned_cvd_df['Age_Category'].apply(extract_lower_bound).astype(int)

cleaned_cvd_df = cleaned_cvd_df.drop(columns=['Age_Category'])

"""## Correlation Testing

Here, we are interested in examining the linear correlation between each feature in our dataset. By identifying highly linear relationships, we would be able to further clean our data by dropping confounding variables. Moreover, by generating a correlation matrix, we can gain more insight into how each feature may be potentially related to each other.
"""

corr_matrix = cleaned_cvd_df.corr(method='pearson')

corr_matrix

"""Let's plot the correlation matrix using `matplotlib` for a cleaner graph with color coding."""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(15,15))
sns.heatmap(
    corr_matrix,
    annot=True,
    fmt=".2f",
    cmap="vlag",
    center=0
)

plt.title("Feature Correlation Matrix")
plt.show()

"""Next, we wanted to identify feature pairs that have an absolute correlation of greater than 0.8. This threshold represents pairs that are highly linearly correlated, which may imply unnecessary variables."""

# ignore self‐correlations
corr_pairs = (
    corr_matrix.where(~np.eye(corr_matrix.shape[0],dtype=bool))
               .stack()
               .abs()
               .sort_values(ascending=False)
)

high_corr = corr_pairs[corr_pairs > 0.8]

high_corr

"""Since the two variables that are highly correlated are BMI and body weight, we decided to drop the BMI column. Logically, this high correlation makes sense due to the formula for BMI:

$BMI = \frac{weight}{height^2}$

Thus, we can safely drop BMI, as we already have weight and height data.
"""

cleaned_cvd_df = cleaned_cvd_df.drop(columns=['BMI'])

"""## Train, Test, Val Splits

Here, we will create the train, test, and validation splits. We will use the train and test dataset for normal train, and we will use the validation dataset for hyperparameter testing. For models that don't require hyperparameter testing, we will group the validation dataset n with the training dataset.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

X = cleaned_cvd_df.drop(columns=['Heart_Disease'])
y = cleaned_cvd_df['Heart_Disease']

X_trainval, X_test, y_trainval, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_trainval, y_trainval, test_size=0.2, random_state=42, stratify=y_trainval
)

"""## Data Upsampling/Downsampling

With such a large class imbalance between those with heart disease and those without, we chose to explore how to incorporate upsampling/downsampling into our models. We hope to alleviate the class imbalance issue by increasing those with CVD, decreasing those without.

We chose to use SMOTEENN as it is a hybrid between purely upsampling (SMOTE) and purely downsampling (Random Sampling). SMOTEENN is simply the SMOTE model with Edited Nearest Neighbours (ENN) to help prune afterwards.

SMOTE oversamples the minority class by creating new “synthetic” examples along the line segments joining each minority sample to its k nearest minority neighbors. Doing so, we reduce the risk of overfitting from just simple duplications of the minority dataset.

On the other hand, the ENN checks if a label disagrees with the majority of its k nearest neighbors; if it is, it'll be removed. By using ENN, we are able to also undersample the majority class via the ENN cleanup logic, removing noisy datapoints that may throw the classification model and creating cleaner distinctings between the two classes.
"""

# Uncomment if you want to regenerate the synthetic dataset

from imblearn.combine import SMOTEENN

smote_enn = SMOTEENN(random_state=42)
X_train_enn, y_train_enn = smote_enn.fit_resample(X_train, y_train)

# df_enn = pd.concat(
#     [
#         pd.DataFrame(X_train_enn, columns=X_train.columns),
#         pd.Series(y_train_enn, name='Heart_Disease')
#     ],
#     axis=1
# )

# # 2. Dump to CSV in your working directory
# csv_path = "train_enn.csv"
# df_enn.to_csv(csv_path, index=False)

"""Since we have already pre-computed the synthetic traning data, we can simply read the csv file here."""

# enn_df = pd.read_csv('/content/drive/MyDrive/train_enn.csv')

# X_train_enn = enn_df.drop(columns=['Heart_Disease'])
# y_train_enn = enn_df['Heart_Disease']

X_train_enn.shape

y_train.value_counts()

y_train_enn.value_counts()

"""# Part 5: Baseline Modelling

We will now train a simple Logistic Regression model as an attempt to predict whether a patient would have heart disease or not. We first trained the model on the original training dataset. Then, we also trained the model using our synthetic data from SMOTEENN.

Although the accuracy of the original dataset is higher, we can clearly from the classification_report that the model relying on the SMOTEENN dataset performed much better in terms of the F-1 score for the 1 class.

## Logistic Regresssion with Original Dataset
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split

"""Since we don't need the validation dataset for Logistic Regression, we can simply merge it back into the training dataset for more datapoints."""

X_train_log = pd.concat([X_train, X_val], axis=0).reset_index(drop=True)
y_train_log = pd.concat([y_train, y_val], axis=0).reset_index(drop=True)

model = LogisticRegression(max_iter=1000)
model.fit(X_train_log, y_train_log)

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("-" * 10)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("-" * 10)
print("Classification Report:\n", classification_report(y_test, y_pred))

coef_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': model.coef_[0]
})

print(coef_df)
print("\nIntercept:", model.intercept_[0])

"""Based on the logistic regression results, the model achieves a high overall accuracy of approximately 91.9%. However, this number is somewhat misleading due to the significant class imbalance in the dataset—only a small fraction of individuals actually have heart disease. The confusion matrix and classification report reveal that the model performs well at identifying individuals without heart disease (class 0), but fails to capture most of the individuals with heart disease (class 1). Specifically, the recall for the positive class is only 0.05, meaning the model correctly identifies just 5% of actual heart disease cases. While the precision for the positive class is 0.49, indicating that nearly half of the predicted positives are correct, the overall ability to detect heart disease is extremely limited. This suggests that the model is biased toward predicting the majority class and is not well-calibrated for applications where detecting the minority class (heart disease) is critical.

Looking at the learned coefficients provides insight into the factors most associated with heart disease in this dataset. Being male has the largest positive coefficient, suggesting a strong association with higher risk. Similarly, variables such as depression, arthritis, and smoking history are also positively associated with heart disease, aligning with known clinical risk factors. On the other hand, features like having had a recent checkup and better reported general health are associated with a lower risk, as indicated by their negative coefficients. Some dietary and biometric variables, such as BMI, weight, and vegetable consumption, have relatively small coefficients, suggesting they play a less direct role in the model’s decision-making. Overall, while the coefficients make intuitive sense and align with medical understanding, the model's poor recall highlights the need for either better sampling techniques (e.g. oversampling the minority class) or alternative modeling approaches more suited to imbalanced data.

## Logistic Regresssion with Synthetic Dataset
"""

X_train_enn_log = pd.concat([X_train_enn, X_val], axis=0).reset_index(drop=True)
y_train_enn_log = pd.concat([y_train_enn, y_val], axis=0).reset_index(drop=True)

model_enn = LogisticRegression(max_iter=1000)
model_enn.fit(X_train_enn_log, y_train_enn_log)

y_pred_enn = model_enn.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred_enn))
print("-" * 10)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_enn))
print("-" * 10)
print("Classification Report:\n", classification_report(y_test, y_pred_enn))

coef_df = pd.DataFrame({
    'Feature': X_train_enn.columns,
    'Coefficient': model_enn.coef_[0]
})

coef_df_abs = coef_df.reindex(coef_df['Coefficient'].abs().sort_values(ascending=False).index)
coef_df_abs = coef_df_abs.reset_index(drop=True)

print(coef_df_abs)
print("\nIntercept:", model_enn.intercept_[0])

"""The model achieves 76.3 % accuracy on the hold-out set, but this masks the 8 % prevalence of heart disease.  In practice it labels 96 % of healthy individuals correctly (TN = 44 011, FP = 12 766) and captures 63 % of actual cases (TP = 3 139, FN = 1 855), yielding a 20 % precision and 0.30 F₁ for the positive class.  In contrast, the majority class enjoys an F₁ of 0.86, reflecting the model’s bias toward predicting “no disease.”  

Inspecting the coefficients highlights that recent checkups (–2.10), better self-rated health (–1.35) and even flags for prediabetes (–1.88) or other cancers (–1.14) are most protective—likely signaling more intensive medical surveillance—while being male (+0.27), taller (+0.02 per cm) and heavier (+0.004 per kg) modestly increase risk.  Some surprising negatives (e.g. depression, diabetes) suggest coding or care-seeking artifacts.  To make this clinically useful, consider shifting the classification threshold to trade precision for recall, applying class-weighted or focal-loss training to penalize missed cases, and revisiting feature encodings or adding interaction terms before exploring more flexible models.

# Part 6: Additional Feature Engineering and Analysis

## K-Means

With so many patient data, it comes naturally to segment the population into various personas for easier analysis. Initially, we hoped to draw meaningful insights regarding each pateint segments that would aid our model design. However, there were large similarities between some clusters, leading us to only extract their respective cluster as another feature for further training.

More importantly, we must drop the `Heart_Disease` column from our dataset because we will be using the clusters as indicator features for our other models. If we do not remove `Heart_Disease`, we may be contaminating our target feature (`Heart_Disease`) with our input features (`cluster`).

Initially, we did try to include the `Heart_Disease` column for the clustering step. However, that led to a near perfect F-1 score for those with heart disease. Thus, it's extremely unwise to "bake in" the target feature into our cluster classification.
"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

"""For K-means, we must first scale the data since the model is **scale variant**. Without scaling the data, our cluster would easily be based on features with higher magnitudes. Note that we drop the `Heart_Disease` feature, so that we can use cluster information in future modelling without leaking.

We can scale the dataset by utilizing scikit-learn's StandardScaler.
"""

k_means_df = cleaned_cvd_df.copy()
k_means_df = k_means_df.drop(columns=['Heart_Disease'])

scaler = StandardScaler()
scaled_data = scaler.fit_transform(k_means_df)

"""One key parameter for K-means clustering is the number of the clusters --- or the $k$ parameter. A common technique to determine the adequate $k$ value is to generate an "elbow plot" that would help us identify the number of clusters that balances the model complexity and inertia within each cluster.

As we can see from the plot, the optimal number of clusters is 6. With each additional clusters, the marginal gain drops significantly.
"""

inertia = []
k_values = range(1, 16)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)

# Plot the elbow graph
plt.figure(figsize=(8, 5))
plt.plot(k_values, inertia, marker='o')
plt.xticks(k_values)
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.grid(True)
plt.show()

"""With the number of clusters in mind, we can dive deeper into what each clusters represents."""

kmeans = KMeans(n_clusters=6, random_state=42)
kmeans.fit(scaled_data)

k_means_df['Cluster'] = kmeans.labels_

"""Since there are over 20 columns, it doesn't make sense to plot each cluster in that dimension. Thus, we can employ PCA to help transform the data into two dimensions while still capturing as much variance as possible. This method allows us to visualize the clusters in a 2D plot.

Note that we are not applying PCA to the original cleaned dataset --- only temporarily for visualization purposes.

As we can see from the clusters, there are discernible distinctions between each clusters with minimal overlap between clusters 1-3.
"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)

plt.figure(figsize=(10, 6))
scatter = plt.scatter(
    pca_data[:, 0],
    pca_data[:, 1],
    c=k_means_df['Cluster'],
    cmap='tab20',
    s=50
)

plt.legend(
    *scatter.legend_elements(),
    title="Cluster",
    loc='upper right'
)

plt.title('Patient Clusters Visualized with PCA')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.grid(True)
plt.show()

"""Now with each clusters, we want to analyze the defining persona that represents the segment. We can do so by analzying the centroids of each cluster and the features that are highest and lowest compared the population averages."""

cluster_profiles = k_means_df.groupby('Cluster').mean()

overall_mean = k_means_df.drop(columns=['Cluster']).mean()

deviation = cluster_profiles.subtract(overall_mean, axis=1)

for cl in deviation.index:
    print(f"\nCluster {cl} (n={len(k_means_df[k_means_df['Cluster']==cl])} patients):")

    top_high = deviation.loc[cl].sort_values(ascending=False).head(3)
    print(" - Highest-than-avg features:")
    for feat, diff in top_high.items():
        print(f"    - {feat}: +{diff:.2f}")

    top_low = deviation.loc[cl].sort_values().head(3)
    print(" - Lowest-than-avg features:")
    for feat, diff in top_low.items():
        print(f"    - {feat}: {diff:.2f}")

"""We can analyze each segments as follows:

**Cluster 0**: Younger, taller-and-heavier individuals with a particularly unhealthy diet.
* Demographics & body habitus:
  * Age: ~7 years younger than the cohort average
  * Height & weight: ~9 cm taller and ~8.6 kg heavier than average
* Dietary habits:
  * High fried-potato consumption (+1.75)
  * Low fruit (–2.66) and green-vegetable (–1.18) intake
* Implications: This is a youthful, physically larger group whose dietary profile (high fried potatoes, low produce) suggests risk for future metabolic issues.

-----------------------------------------------

**Cluster 1**: Middle-aged, overweight patients with elevated diabetes prevalence and generally poor produce intake.

* Demographics & body habitus:
  * Age: ~9 years older than average
  * Weight: ~10 kg above average
* Health conditions:
  * Diabetes: +0.87 (i.e. higher-than-average prevalence)
* Dietary & lifestyle:
  * Low alcohol (–2.41), fruit (–1.60), and green-veg (–1.32) consumption
* Implications: Overweight, diabetic subgroup with low produce but also low alcohol; likely under medical management for diabetes.

-----------------------------------------------

**Cluster 2**: Older patients with slightly higher cancer rates, modest produce intake, and below-average body size.

* Demographics & body habitus:
  * Age: ~13 years older than average
  * Height & weight: ~1.5 cm and ~1.6 kg below average
* Health conditions:
  * Other cancer: +0.90 prevalence
* Dietary & lifestyle:
  * Slightly high fruit intake (+0.61)
  * Low fried potatoes (–0.91)
* Implications: This older, leaner subgroup has elevated cancer prevalence; their relatively better fruit intake may reflect some health awareness.

-----------------------------------------------

**Cluster 3**: Older, shorter and lighter individuals with higher arthritis and smoking history, but low fruit intake.

* Demographics & body habitus:
  * Age: ~11 years older than average
  * Height & weight: ~4.5 cm and ~2.8 kg below average
* Health conditions:
  * Arthritis: +0.52 prevalence
  * Smoking history: +0.14
* Dietary:
  * Low fruit intake (–1.52)
* Implications: An aging, smaller-stature cluster burdened by arthritis and past smoking, with poor fruit consumption.

-----------------------------------------------

**Cluster 4**: Younger, lean individuals with the healthiest produce intake and slightly better self-rated health.

* Demographics & body habitus:
  * Age: ~7 years younger than average
  * Height & weight: ~7–12 units below average
* Health & lifestyle:
  * High fruit (+4.85) and green-veg (+3.06) consumption
  * General health: +0.44 self-rating above average
* Implications: This “health-conscious” cluster is lean, young, and eats lots of produce; they report better general health.

-----------------------------------------------

**Cluster 5**: Very young, tall individuals who get regular checkups but eat few fruits and vegetables.

* Demographics & body habitus:
  * Age: ~10 years younger than average
  * Height: +2.4 cm above average
* Preventive care & diet:
  * Checkup visits: +5.41 (much more frequent)
  * Low fruit (–4.38) and green-veg (–2.52) consumption
* Implications: Though small, this group is notable for high engagement with checkups.

With our in-depth analysis of each patient segments, we can now see why there is an overlap between clsuters 1-3. All three of the clusters exhibit patients with poor health and lifestyles. Albeit, they all have different variations of bad health (overweight, cancer, arthritis).

Since each cluster personas are too complex to capture with new variables due to their respective nuances. Therefore, we chose to compromise and add a one hot encoding for each of the 6 clusters to our cleaned dataset; these clusters should help our models indenitfy each patient groups.
"""

from sklearn.preprocessing import OneHotEncoder

cleaned_cvd_df['Cluster'] = kmeans.labels_

encoder = OneHotEncoder(sparse_output=False, dtype=int)
cluster_arr = cleaned_cvd_df['Cluster'].to_numpy().reshape(-1, 1)
onehot = encoder.fit_transform(cluster_arr)

cols = encoder.get_feature_names_out(['Cluster'])

onehot_df = pd.DataFrame(onehot, columns=cols, index=cleaned_cvd_df.index)
cleaned_cvd_df = pd.concat([cleaned_cvd_df.drop('Cluster', axis=1), onehot_df], axis=1)

"""# Part 8: Random Forest

### Why Use a Random Forest Over Logistic Regression?

While Logistic Regression provides a simple and interpretable baseline, it assumes linearity and often underfits complex patterns.

We use Random Forests to model non-linear interactions between behavioral features and cardiovascular disease (CVD) status.

Our dataset is highly imbalanced — most samples are from individuals without CVD.

 To handle this:
- We compute class weights inversely proportional to class frequencies.
- We explicitly tune the `class_weight` hyperparameter during Randomized Search.
- Our evaluation focuses on the **F1 score**, which balances precision and recall, making it suitable for imbalanced classification.

By introducing tree-based complexity and using class-sensitive weighting, we aim to improve the detection of minority class (CVD-positive) samples while controlling overfitting.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
SEED=42

"""## Hyperparameter Tuning: RandomizedSearchCV

### StratifiedKFold

When tuning hyperparameters for a classification model like `RandomForestClassifier` using `RandomizedSearchCV`, we want to evaluate candidate models on representative subsets of the data. This is especially important when the dataset is imbalanced.

Using `StratifiedKFold` ensures that each fold used during cross-validation maintains the same class distribution as the full dataset. This prevents situations where some folds might contain too few examples of the minority class (patients with CVD), which would lead to unreliable validation scores and potentially suboptimal hyperparameter selection.
"""

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)

"""### Custom Class Weights
To address class imbalance, we  tune a custom `class_weight` scaling as a hyperparameter. By increasing the weight of the positive class during training, we can encourage the model to place greater emphasis on correctly predicting minority class samples.

We first compute base class weights from the training + validation data using `class_weight='balanced'`. The utility function, `get_scaled_class_weight` allows us to search over different multipliers during hyperparameter tuning to find the balance that yields the best model performance.
"""

# Base class weights from our train/validation dataset
weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_enn_log), y=y_train_enn_log)
base_class_weight = {0: weights[0], 1: weights[1]}

# Returns a class weight with the positive weight scaled by multiplier
def get_scaled_class_weight(multiplier):
    return {0: base_class_weight[0], 1: base_class_weight[1] * multiplier}

"""### Parameter Grid and Tuning

We used `RandomizedSearchCV` to tune the following hyperparameters:

- `n_estimators`: Number of trees. More trees reduce variance but increase computation.
- `max_depth`: Limits how deep each tree can grow. Prevents overfitting.
- `min_samples_leaf`: Minimum samples at a leaf node. Larger values smooth the model. Reduces overfitting to rare noise at leaves.
- `max_features`: Number of features considered when splitting, controlling randomness.
- `class_weight`: A custom dictionary scaling up the positive class to prioritize CVD cases.

We optimize for F1 score, not accuracy, since the dataset is imbalanced. We also use RandomizedSearchCV over GridSearchCV to allow for faster tuning while covering key combinations.
"""

# Parameters our RandomizedSearchCV will search over
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 10, 20],
    'min_samples_leaf': [1, 5, 10],
    'max_features': ['sqrt'],
    'class_weight': [get_scaled_class_weight(m) for m in [1.0, 1.5, 2.0]]
}

# Search parameters, scoring based on f1
rf = RandomForestClassifier(random_state=42)
randomized_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_grid,
    n_iter=10,
    scoring='f1',
    cv=cv,
    n_jobs=-1,
    verbose=2,
    random_state=42
)

randomized_search.fit(X_train_enn_log, y_train_enn_log)

best_params = randomized_search.best_params_
print(best_params)

"""## Analyzing Our Final Random Forest Classifier
### Complexity & Regularization

Random Forest naturally guards against overfitting via:
- **Ensemble averaging**: combining many trees reduces model variance
- **Bootstrapping**: each tree is trained on a different sample of the data (with replacement)
- **Feature randomness**: only a subset of features is considered per split

To prevent underfitting:
- We allow deeper trees and lower leaf/split thresholds during tuning.
- Complexity is introduced gradually via `RandomizedSearchCV`.

This balance helps the model generalize while still capturing rare CVD signals.

### Bootstrapping in Random Forest

Each decision tree in the forest is trained on a bootstrapped sample This introduces variance between trees and is a key source of model stability.

"""

best_rf = randomized_search.best_estimator_
rf_predictions = best_rf.predict(X_test) # probability estimates for all
y_prob = best_rf.predict_proba(X_test)[:,1]  # probability estimates for positive class
y_pred = (y_prob >= 0.5).astype(int)

"""### Initial Performance Look

Our Random Forest classifier achieved strong overall performance on the held-out test set, with a weighted F1 score of 0.85 and an accuracy of 81%.

However, due to class imbalance, accuracy is not a reliable metric. A more nuanced look shows that the model performed well in identifying CVD cases, with a recall of 0.66 for the positive class. This means it successfully identified about two-thirds of all true CVD cases. The negative class was classified with high precision (0.96) and recall (0.83), reflecting strong performance in ruling out non-CVD cases. Overall, the macro-averaged recall was 0.74, indicating decent sensitivity across both classes.

Despite this, the model suffers from low precision for the positive class (0.25), meaning many non-CVD cases were incorrectly flagged as CVD. This tradeoff—high recall at the cost of false positives—is intentional, as false negatives are more dangerous in medical screening. The confusion matrix shows 1,713 false negatives and 9,868 false positives. These results suggest the model could be valuable as a screening tool, where sensitivity is prioritized over specificity.
"""

print(classification_report(y_test, rf_predictions))
print(confusion_matrix(y_test, rf_predictions))

"""### Threshold Optimization

Our classifier works as so: The random forest classifier returns a probability for our target (having CVD). At default, if this probability is above 0.5, it returns Yes, and otherwise, it returns No. However, it's possible to optimize the threshold manually. We can plot the (positive) precision, recall, and f1-score, against the threshold in order to find the threshold that optimizes our model's evaluation measure. Since we're using the positive f1-score, we'll optimize threshold to return the maximum f1.
"""

precision, recall, thresholds = precision_recall_curve(y_test, y_prob)

# Compute F1 scores
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)

# Find best threshold (max F1)
best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx]
best_f1 = f1_scores[best_idx]

plt.plot(thresholds, precision[:-1], label='Precision')
plt.plot(thresholds, recall[:-1], label='Recall')
plt.plot(thresholds, f1_scores[:-1], label='F1 Score', linestyle='--')
plt.axvline(x=best_threshold, color='red', linestyle=':', label=f'Best Threshold = {best_threshold:.2f}')
plt.scatter(best_threshold, best_f1, color='red', zorder=5)

plt.text(best_threshold, best_f1 + 0.02, f'F1={best_f1:.2f}', ha='center', color='red')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.title('Precision, Recall, and F1 vs Threshold')
plt.legend()
plt.grid()
plt.show()

# Apply the best threshold (from the PR curve analysis)
y_pred_thresh = (y_prob >= best_threshold).astype(int)

# Re-evaluate with thresholded predictions
from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(y_test, y_pred_thresh))
print(confusion_matrix(y_test, y_pred_thresh))

"""#### Impact of Threshold Optimization

We compared the model’s performance at the default classification threshold (0.5) and the optimized threshold (0.58), which was selected to maximize the F1 score on the validation set. The results are summarized below:

| Metric     | Default (Threshold=0.5) | Tuned (Threshold=0.58) | Change         |
|----------------------|----------------|----------------|----------------|
| Precision (CVD)      | 0.25           | 0.28           | ↑ Slightly     |
| Recall (CVD)         | 0.66           | 0.53           | ↓ Moderately   |
| F1 Score (CVD)       | 0.36           | 0.37           | ↑ Slightly     |
| Accuracy (Overall)   | 0.81           | 0.85           | ↑ Noticeably   |
| False Positives      | 9,868          | 6,713          | ↓ Significantly|
| False Negatives      | 1,713          | 2,337          | ↑ Significantly|

The threshold adjustment improves **precision and overall accuracy** by reducing the number of false positives — a common source of unnecessary follow-up tests in clinical screening. However, this comes at the cost of **lower recall**, meaning more CVD cases are missed.

Whether this trade-off is acceptable depends on the use case. In high-stakes screening settings where catching all possible CVD cases is critical, the higher recall at threshold 0.5 may be preferred. For settings where false positives are costly or burdensome, the threshold-optimized version may be better suited.

This trade-off should be considered based on the goals of clinical applications, as there's no one best answer in this case.

### ROC Curve

This plot shows the ROC curve for our classifier with **both the default threshold** and the **optimized threshold** marked.

- The **blue dot** corresponds to the default threshold of 0.50. It yields **higher recall** but also a **higher false positive rate**.
- The **red dot** corresponds to our custom threshold of 0.58, selected to optimize F1 score. It achieves **fewer false positives** but at the cost of some missed positives.

Visualizing both operating points helps us choose the best threshold based on clinical priorities — whether we value catching more true CVD cases or minimizing false alarms.
"""

fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

def get_fpr_tpr(y_predictions):
    tn, fp, fn, tp = confusion_matrix(y_test, y_predictions).ravel()
    fpr = fp / (fp + tn)
    tpr = tp / (tp + fn)
    return fpr, tpr

fpr_thresh, tpr_thresh = get_fpr_tpr(y_pred_thresh)
fpr_default, tpr_default = get_fpr_tpr(y_pred)

plt.figure()
plt.scatter(fpr_thresh, tpr_thresh, color='red', zorder=5, label=f'Threshold = {best_threshold:.2f}')
plt.scatter(fpr_default, tpr_default, color='blue', zorder=5, label='Default Threshold = 0.5')
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # random guess line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""### One Final Look

Our Random Forest Classifier was able to achieve a high weighted f1-score of 0.85. However, because of the imbalance of our dataset, we are primarily evaluating our models on positive f1-score. In our default threshold model, we had a f1-score of 0.36, and in our optimized threshold model, we had a f1-score of 0.37. As discussed, we came across trade-offs of positive recall and precision when comparing these.

With Random Forest Classifiers, we are able to look into what went into the decision, which can be valuable for interpretability.

The most influential features in the model were **Age**, **General Health**, **Weight**, and several diet-related variables (e.g., fruit and vegetable consumption, alcohol use, and fried food intake). Notably, **Cluster 4**, a group identified via K-means clustering, was also among the top predictors. This cluster characterized patients with healthier-than-average diets and lower weight, age, and height — consistent with reduced CVD risk. Its importance in the model suggests that behavioral patterns captured via unsupervised learning can enhance predictive performance while preserving interpretability.
"""

print("Default Threshold\n----------------------------------------------")
print(classification_report(y_test, y_pred))

print("Optimized Threshold\n----------------------------------------------")
print(classification_report(y_test, y_pred_thresh))

importances = best_rf.feature_importances_
feature_names = X_test.columns  # or use X.columns if same structure

# Create a sorted dataframe
feat_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feat_df = feat_df.sort_values('Importance', ascending=False)

print(feat_df.head(10))

"""# Part 9: Neural Network

### Why Use a Neural Network Over a Random Forest?

Random Forests are effective for capturing non-linear patterns and provide feature importance insights, but they are limited in their capacity to model complex interactions at scale, especially when dealing with high-dimensional or highly correlated features.

We use a feedforward **Artificial Neural Network (ANN)** to capture richer, hierarchical representations of the behavioral features that may influence cardiovascular disease (CVD) risk.

Neural networks offer several advantages:
- They can model intricate non-linear relationships beyond the reach of tree-based methods.
- With sufficient depth and regularization, they generalize well to subtle patterns, including interactions across multiple feature combinations.
- They naturally handle continuous input features without requiring discretization or heavy preprocessing.

As with Random Forests, we address class imbalance by:
- Using a weighted binary cross-entropy loss function, with weights set inversely to class frequencies.
- Tuning architectural and training hyperparameters (e.g. number of layers, learning rate, dropout rate, class weights) using **Randomized Search** guided by **F1 score**.

By leveraging the function-approximating power of neural networks, we aim to improve sensitivity to minority class (CVD-positive) cases while preserving generalization and stability across folds.

##Data Loading

First we import the relevent libraries and create proper data containers for PyTorch.
"""

import torch.nn as nn
import torch
import torch.optim as optim
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import matplotlib.pyplot as plt
import random
import itertools
import copy
import numpy as np
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"

X = cleaned_cvd_df.drop(columns=['Heart_Disease'])
y = cleaned_cvd_df['Heart_Disease']

X_trainval, X_test, y_trainval, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_trainval, y_trainval, test_size=0.2, random_state=42, stratify=y_trainval
)

class CVDataset(Dataset):
    def __init__(self, X_df, y_series):
        self.x = torch.tensor(X_df.to_numpy(dtype=np.float32), dtype=torch.float32)
        self.y = torch.tensor(y_series.to_numpy(dtype=np.float32), dtype=torch.float32)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

train_ds = CVDataset(X_train, y_train)
val_ds   = CVDataset(X_val, y_val)
test_ds  = CVDataset(X_test, y_test)
train_enn_ds = CVDataset(X_train_enn, y_train_enn)

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False)
test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False)
train_enn_loader = DataLoader(train_enn_ds, batch_size=32, shuffle=True)

class_counts = y_train.value_counts()
pos_w = torch.tensor([class_counts[0] / class_counts[1]], device=device)

"""## Defining the Network

Our neural network architecture is designed to model complex patterns in behavioral and clinical features related to cardiovascular disease (CVD). It consists of an input layer for 26 features, followed by three hidden layers with adjustable sizes.

- **Deep Non-Linear Modeling:**  Each hidden layer is followed by a ReLU activation, allowing the network to learn complex, non-linear relationships that simpler models may miss.

- **Progressive Abstraction:** The layered structure helps the network build hierarchical representations, moving from low-level patterns to more abstract interactions relevant for predicting CVD.

- **Dimensional Compression:**  The decreasing hidden layer sizes act as a bottleneck, filtering out noise and focusing the model on key predictive features.

- **Balanced Complexity:**  The architecture is deep enough to capture subtle interactions, yet compact enough to avoid overfitting, especially with regularization and class-weighted loss.

This feedforward network provides the flexibility and representational power needed to handle the noisy, imbalanced, and potentially non-linear nature of our heart disease dataset.
"""

class FeedforwardNN(nn.Module):
    def __init__(self, d_in=26, hidden_dims=[128,64,32]):
        super().__init__()
        layers = []
        last_dim = d_in
        for h in hidden_dims:
            layers.append(nn.Linear(last_dim, h))
            layers.append(nn.ReLU())
            last_dim = h
        layers.append(nn.Linear(last_dim, 1))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x).squeeze(1)

"""## Hyperparameter Tuning

To optimize neural network performance, we conduct a **randomized hyperparameter search** over a predefined grid. Rather than exhaustively evaluating all combinations, we randomly sample a subset, which offers a more efficient way to explore the hyperparameter space given computational constraints.

Our search space includes:
- **Learning rate (`lr`)**: `[3e-4, 1e-4, 5e-5, 3e-5]`
- **Hidden layer architectures (`hidden_dims`)**: Various configurations from deep to shallow, e.g. `[128, 64, 32]`, `[32, 16]`, etc.
- **Weight decay (`weight_decay`)**: `[0, 1e-5, 1e-4]` to control L2 regularization
- **Classification threshold (`threshhold`)**: `[0.3, 0.4, 0.5, 0.6, 0.7]` to optimize the F1 tradeoff between precision and recall

For each randomly sampled configuration:
- We train the network for 8 epochs.
- We evaluate on a validation set using **F1 score**, which is robust to our class imbalance.
- We apply **class-weighted binary cross-entropy loss**, where the positive class (CVD-positive) receives a higher weight proportional to the imbalance ratio.
- The final score is computed as the average F1 score over the last 3 validation epochs.

This approach allows us to jointly tune architectural, optimization, and decision-threshold parameters. By selecting the configuration with the highest validation F1, we prioritize models that best balance sensitivity and specificity for detecting CVD.
"""

param_grid = {
    "lr": [3e-4, 1e-4, 5e-5, 3e-5],
    "hidden_dims": [
        [128, 64, 32],
        [32, 16],
        [64, 32],
        [128, 64],
        [64, 32, 16],
    ],
    "weight_decay": [0, 1e-5, 1e-4],
    "threshhold" : [0.3, 0.4, 0.5, 0.6, 0.7]
}

np.random.seed(42)

def random_sample_grid(param_grid, n_samples=10):
    keys = list(param_grid.keys())
    samples = []
    random.seed(42)
    for _ in range(n_samples):
        sample = {k: random.choice(param_grid[k]) for k in keys}
        samples.append(sample)
    return samples

sampled_params = random_sample_grid(param_grid)

print(f"Randomized {len(sampled_params)} hyperparameter sets:")
for i, p in enumerate(sampled_params):
    print(f" {i}: {p}")

def train_and_eval_model(hparams, epochs=8):
    # Setup model
    model = FeedforwardNN(hidden_dims=hparams['hidden_dims']).to(device)
    optimizer = optim.Adam(model.parameters(), lr=hparams['lr'], weight_decay=hparams['weight_decay'])
    pos_w = torch.tensor([class_counts[0] / class_counts[1]], device=device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_w)
    threshold = hparams['threshhold']

    f1_scores = []

    # add tqdm progress bar over epochs
    for ep in tqdm(range(epochs), desc="Training Progress"):
        model.train()
        for xb, yb in train_enn_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            logits = model(xb)
            loss = criterion(logits, yb)
            loss.backward(); optimizer.step()

        model.eval()
        tp = fp = tn = fn = 0
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device)
                logits = model(xb)
                preds = (torch.sigmoid(logits) > threshold).float()
                tp += ((preds==1)&(yb==1)).sum().item()
                fp += ((preds==1)&(yb==0)).sum().item()
                tn += ((preds==0)&(yb==0)).sum().item()
                fn += ((preds==0)&(yb==1)).sum().item()

        val_precision = tp / (tp + fp + 1e-9)
        val_recall = tp / (tp + fn + 1e-9)
        val_f1 = 2 * (val_precision * val_recall) / (val_precision + val_recall + 1e-9)

        f1_scores.append(val_f1)

    # return avg of last 3
    return sum(f1_scores[-3:]) / 3

results = []
for i, params in enumerate(sampled_params):
    print(f"\nTraining model {i+1}/{len(sampled_params)} with params {params}")
    best_f1 = train_and_eval_model(params)
    print(f"Validation F1 for this set: {best_f1:.4f}")
    results.append((params, best_f1))

best_params, best_f1 = max(results, key=lambda x: x[1])
print("\nBest Hyperparameters:")
print(best_params)
print(f"Best Validation F1: {best_f1:.4f}")

"""## Final Run

After identifying the optimal hyperparameters, we train the model one final time.
"""

best_params = {'lr': 0.0003, 'hidden_dims': [64, 32, 16], 'weight_decay': 0, 'threshhold': 0.7}

# Model
final_model = FeedforwardNN(hidden_dims=best_params['hidden_dims']).to(device)

# Loss
class_counts_final = y_train.value_counts()
pos_w_final = torch.tensor([class_counts_final[0] / class_counts_final[1]], device=device)
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_w_final)

# Optimizer
optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])

threshold = best_params['threshhold']
epochs = 20

final_model

final_train_loss, final_test_loss = [], []
final_train_acc, final_test_acc = [], []
final_train_prec, final_test_prec = [], []
final_train_rec, final_test_rec = [], []
final_train_f1, final_test_f1 = [], []

for ep in range(epochs):
    # ─── train ───
    final_model.train()
    running = 0.0
    tp_train = fp_train = tn_train = fn_train = 0
    for xb, yb in train_enn_loader:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        logits = final_model(xb)
        loss = criterion(logits, yb)
        loss.backward(); optimizer.step()
        running += loss.item() * yb.size(0)

        preds = (torch.sigmoid(logits) > threshold).float()
        tp_train += ((preds==1)&(yb==1)).sum().item()
        fp_train += ((preds==1)&(yb==0)).sum().item()
        tn_train += ((preds==0)&(yb==0)).sum().item()
        fn_train += ((preds==0)&(yb==1)).sum().item()

    train_loss = running / len(train_ds)
    train_acc = (tp_train + tn_train) / (tp_train + tn_train + fp_train + fn_train)
    train_prec = tp_train / (tp_train + fp_train + 1e-9)
    train_rec = tp_train / (tp_train + fn_train + 1e-9)
    train_f1 = 2 * (train_prec * train_rec) / (train_prec + train_rec + 1e-9)

    # ─── test ───
    final_model.eval()
    running_test = 0.0
    tp = fp = tn = fn = 0
    with torch.no_grad():
        for xb, yb in test_loader:
            xb, yb = xb.to(device), yb.to(device)
            logits = final_model(xb)
            running_test += criterion(logits, yb).item() * yb.size(0)

            preds = (torch.sigmoid(logits) > threshold).float()
            tp += ((preds==1)&(yb==1)).sum().item()
            fp += ((preds==1)&(yb==0)).sum().item()
            tn += ((preds==0)&(yb==0)).sum().item()
            fn += ((preds==0)&(yb==1)).sum().item()

    test_loss = running_test / len(test_ds)
    test_acc = (tp + tn) / (tp + tn + fp + fn)
    test_prec = tp / (tp + fp + 1e-9)
    test_rec = tp / (tp + fn + 1e-9)
    test_f1 = 2 * (test_prec * test_rec) / (test_prec + test_rec + 1e-9)

    final_train_loss.append(train_loss)
    final_test_loss.append(test_loss)
    final_train_acc.append(train_acc)
    final_test_acc.append(test_acc)
    final_train_prec.append(train_prec)
    final_test_prec.append(test_prec)
    final_train_rec.append(train_rec)
    final_test_rec.append(test_rec)
    final_train_f1.append(train_f1)
    final_test_f1.append(test_f1)

    print(f"Epoch {ep:02d} | train loss {train_loss:.4f} acc {train_acc:.3f} prec {train_prec:.3f} rec {train_rec:.3f} f1 {train_f1:.3f} || "
          f"test loss {test_loss:.4f} acc {test_acc:.3f} prec {test_prec:.3f} rec {test_rec:.3f} f1 {test_f1:.3f}")

"""Over the course of 20 training epochs, the neural network exhibited stable learning behavior with consistent improvements in precision-recall balance, especially for the minority class (CVD-positive). The training loss steadily declined from 0.995 to 0.920, while the training F1 score rose from 0.320 to 0.366, indicating improved classification of positive cases without overfitting. Accuracy remained relatively stable around 84–85%, which reflects the model's ability to maintain general performance despite the class imbalance.

On the test set, F1 scores improved from 0.353 in the first epoch to a peak around 0.372 in later epochs, with fluctuations mostly within a ±0.01 range. This plateau suggests that while the model reached a saturation point in learning, it maintained generalization and did not collapse into trivial predictions. Notably, recall remained consistently higher than precision, indicating the model prioritized identifying true CVD cases over avoiding false positives—a desirable property in a health risk prediction setting.

Overall, the ANN achieved better recall and F1 scores than baseline classifiers like logistic regression or unweighted tree-based models, justifying its use. The stability across epochs and alignment between training and test metrics suggest that the model architecture and class weighting strategy are appropriate for the imbalanced classification task at hand. Further improvements could be explored via techniques like dropout regularization, batch normalization, or learning rate scheduling.

## Plotting

The Train vs Test Loss plot shows a consistent decline in training loss across epochs, indicating that the model is steadily minimizing the binary cross-entropy objective. Meanwhile, the test loss remains relatively stable with mild fluctuations. The lack of a significant widening gap between train and test loss suggests that the model is not overfitting, and generalization to unseen data is reasonably well preserved.
"""

# ─── Loss Plot ───────────────────────────────────────────
plt.figure(figsize=(6,4))
plt.plot(range(epochs), final_train_loss, label="Train Loss")
plt.plot(range(epochs), final_test_loss,  label="Test Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss (BCE)")
plt.title("Train vs Test Loss")
plt.xticks(range(0, epochs, 3))
plt.legend()
plt.grid(True)
plt.show()

"""The Train vs Test Accuracy plot shows that training accuracy stays steady around 84-85%, while test accuracy is more erratic. The sharp dips and peaks in test accuracy—despite stable training—are likely due to accuracy being dominated by the majority class, making it less reliable for evaluating performance on imbalanced datasets. However, it should be noted that we do not think accruacy is a good metric for this dataset, and we think that the F1 score is much more appropriate."""

# ─── Accuracy Plot ───────────────────────────────────────
plt.figure(figsize=(6,4))
plt.plot(range(epochs), final_train_acc, label="Train Accuracy")
plt.plot(range(epochs), final_test_acc,  label="Test Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Train vs Test Accuracy")
plt.xticks(range(0, epochs, 3))
plt.legend()
plt.grid(True)
plt.show()

"""In the Train vs Test Precision plot, training precision exhibits a modest upward trend over time. However, test precision fluctuates considerably across epochs, showing instability in the model's ability to limit false positives on unseen data. This variance may reflect the class imbalance in the dataset or sensitivity to small decision boundary shifts, implying a need for further regularization or threshold tuning."""

# ─── Precision Plot ──────────────────────────────────────
plt.figure(figsize=(6,4))
plt.plot(range(epochs), final_train_prec, label="Train Precision")
plt.plot(range(epochs), final_test_prec,  label="Test Precision")
plt.xlabel("Epoch")
plt.ylabel("Precision")
plt.title("Train vs Test Precision")
plt.xticks(range(0, epochs, 3))
plt.legend()
plt.grid(True)
plt.show()

"""The Train vs Test Recall plot reveals more stability in the training recall, which hovers around 0.54 after an initial jump. In contrast, test recall varies widely, ranging from around 0.40 to above 0.70. These spikes suggest the model is intermittently very sensitive to positive cases but lacks consistency, possibly due to underrepresentation of the positive class or instability during evaluation."""

# ─── Recall Plot ──────────────────────────────────────
plt.figure(figsize=(6,4))
plt.plot(range(epochs), final_train_rec, label="Train Recall")
plt.plot(range(epochs), final_test_rec, label="Test Recall")
plt.xlabel("Epoch")
plt.ylabel("Recall")
plt.title("Train vs Test Recall")
plt.xticks(range(0, epochs, 3))
plt.legend()
plt.grid(True)
plt.show()

"""Lastly, the Train vs Test F1 Score plot is perhaps the most telling. Both train and test F1 scores gradually increase and plateau, with the test F1 remaining slightly above the training curve throughout. The relative alignment of the two suggests that the model is improving in balancing precision and recall without significant overfitting. The stability of test F1 after early epochs indicates convergence and reinforces that F1 is a more appropriate metric than accuracy for this task."""

# ─── F1 Plot ──────────────────────────────────────
plt.figure(figsize=(6,4))
plt.plot(range(epochs), final_train_f1, label="Train F1")
plt.plot(range(epochs), final_test_f1,  label="Test F1")
plt.xlabel("Epoch")
plt.ylabel("F1")
plt.title("Train vs Test F1")
plt.xticks(range(0, epochs, 3))
plt.legend()
plt.grid(True)
plt.show()

"""# Part 10: Approach and Methods

Our main goal was to create predictive classification strategies for cardiovascular disease, based on behavioral and lifestyle factors.

### Data Exploration and Preprocessing

During **data loading and preprocessing**, followed by **exploratory data analysis (EDA)**, we found that our dataset was **highly imbalanced**—as expected for real-world health data. This imbalance became apparent in baseline models like **logistic regression**, where high overall accuracy or weighted F1 scores could obscure **poor performance on the minority (positive) class**, often yielding **low precision or recall** for predicting cardiovascular disease.

### Feature Engineering

To strengthen our models, we applied a range of **feature engineering techniques**:

- **One-Hot Encoding** for nominal categorical variables.
- **Ordinal Encoding** for ordinal categorical variables.
- **Elimination of highly correlated features**, to reduce multicollinearity and noise.
- **K-Means Clustering** as an unsupervised learning step to identify hidden structure in the data. The resulting cluster labels were used as additional features.

### Addressing Class Imbalance

Given the severe class imbalance, we tested several strategies to improve model fairness and performance:

- **Random Forest Classifier**:
  - Applied **class weighting** during training via `RandomizedSearchCV`.
  - Explored **threshold optimization**, adjusting the decision boundary to balance sensitivity and specificity based on downstream clinical priorities.

- **Neural Network**:
  - Used a **custom class-weighted binary cross-entropy loss**, increasing the penalty for misclassifying positive (CVD) cases.
  - Tuned architecture and training hyperparameters, such as learning rate, hidden dimensions, dropout, and weight decay.

- **Resampling Techniques**:
  - Implemented **SMOTE (Synthetic Minority Over-sampling Technique)** combined with **Edited Nearest Neighbors (ENN)** for hybrid upsampling and noise reduction.
  - This approach generated synthetic positive-class examples while pruning ambiguous or noisy samples.

### Pipeline Overview

The complete end-to-end pipeline consisted of:

```
Data Loading
  ↓
Feature Engineering
  - One-Hot Encoding
  - Ordinal Encoding
  - Remove Correlated Features
  ↓
Additional Features
  - K-Means Clustering
  - SMOTE + ENN (Synthetic Balancing)
  ↓
Model Training
  → Random Forest:
      - RandomizedSearchCV with Class Weights
      - Threshold Optimization (Optional based on Clinical Application)
  → Neural Network:
      - Class-Weighted Loss
      - Custom Hyperparameter Tuning
```

Each component was designed to integrate seamlessly, allowing us to experiment with different combinations of preprocessing, resampling, and model-specific strategies to improve performance on this challenging dataset.

# Part 11: Results

### Interpreting the F1 Scores

The **F1 score** is the harmonic mean of precision and recall.

In our chart, we specifically report the **F1 score for the positive class** (i.e., individuals at risk for cardiovascular disease). This choice emphasizes the model's ability to correctly identify and prioritize high-risk cases rather than the majority class (healthy individuals), which is crucial in an imbalanced classification task.

From the results:
- Models that incorporate **sampling techniques** (SMOTE) and more complex classifiers (Random Forests, Neural Nets) yield much higher positive F1 scores than the baseline.
- This indicates better balance between **correct identification of positive cases (recall)** and **avoiding false alarms (precision)**.
- The fact that multiple models reached a plateau around **0.37** suggests that class balancing helped significantly, but further performance gains may require additional features, model architectures, or ensemble approaches.

### Why We Chose Positive F1

We focused on the **positive class F1** because:

1. **Class imbalance**: The dataset has far more negative than positive samples. Accuracy or macro F1 would be misleading.
2. **Real-world impact**: False negatives (failing to detect a CVD case) can have serious health consequences.
3. **Tradeoff clarity**: F1 captures the balance between precision (avoiding false positives) and recall (avoiding false negatives), which is vital for health screening.
4. **Model evaluation fairness**: It ensures we aren’t rewarding models for simply predicting the majority class well.

In sum, positive F1 is a targeted, informative, and fair metric for our problem.
"""

import matplotlib.pyplot as plt

# Positive f1 scores and models
models = [
    'LogReg (Baseline)',
    'LogReg (Synth)',
    'RandomForest (Synth)',
    'RandomForest (Synth, Opt)',
    'NeuralNet (Synth)'
  ]
positive_f1 = [0.11, 0.29, 0.36, 0.37, 0.37]

# Plot
plt.bar(models, positive_f1)
plt.ylabel('F1 Score')
plt.xlabel('Model')
plt.xticks(rotation=60)
plt.title('F1 Scores by Model')
plt.show()

"""# Part 12: Conclusion

## Interpretable Model (Logistic Regression): Feature Importance and Takeaways

In the logistic regression model, feature coefficients reflect the log-odds impact on cardiovascular disease (CVD) risk:

- The most negative coefficients—associated with decreased CVD risk—include:  
  - `Had_Checkup`, `PreDiabetes_Flag`, `General_Health`, and `Gestational_Female_Flag`.  
  These suggest that prior engagement with healthcare (e.g., checkups or early diagnosis) is protective.

- Features with positive coefficients—increasing predicted risk—include:  
  - `Male`, `Age_Lower`, `Weight_(kg)`, and `Height_(cm)`.  
  These align with known demographic and physiological risk factors for CVD.

These interpretable results can inform health practitioners about which lifestyle and health factors to prioritize in interventions or discussions with patients.
"""

coef_df = pd.DataFrame({
    'Feature': X_train_enn.columns,
    'Coefficient': model_enn.coef_[0]
})

coef_df_abs = coef_df.reindex(coef_df['Coefficient'].abs().sort_values(ascending=False).index)
coef_df_abs = coef_df_abs.reset_index(drop=True)

print(coef_df_abs)
print("\nIntercept:", model_enn.intercept_[0])

"""## Non-Interpretable Models (Random Forest & Neural Network): Key Drivers of Performance

**Random Forest:**
- `Age_Lower` was the most important predictor, consistent with age being a dominant CVD risk factor.
- `General_Health` served as a strong proxy for underlying comorbidities.
- `Weight_(kg)` and `Height_(cm)` were also important, reflecting physical condition.
- `Cluster_4`, derived from K-means, showed that unsupervised structure in the data provided additional predictive value. We saw that Cluster 4 was characterized as a group that was more young and with more healthy habits, which could rationalize why this clustering was of value.

**Neural Network:**
- Though less interpretable, performance depended strongly on hyperparameters such as learning rate, hidden layer dimensions, and classification threshold.
- The most critical implementation detail was use of class-weighted loss to address imbalance and improve recall on minority class.
- A high decision threshold (well above 0.5) was necessary to reduce false positives and calibrate predictions for clinical relevance.

These models confirm that both traditional and learned features contribute to accurate predictions, though interpretation is more difficult in neural models.
"""

importances = best_rf.feature_importances_
feature_names = X_test.columns  # or use X.columns if same structure

# Create a sorted dataframe
feat_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feat_df = feat_df.sort_values('Importance', ascending=False)

print(feat_df.head(10))

"""## Modeling Implications for Stakeholders

All three models provide actionable insights for different stakeholders:

- Clinicians and health professionals can use logistic regression results to guide patient education and prioritize risk factors.
- Random forest and neural network models may serve better as backend engines for screening tools, where performance matters more than interpretability.
- Public health agencies may benefit from feature-level insights (e.g., general health, age, exercise) when designing interventions.

## Model Limitations

- The dataset is synthetic and may not fully capture the variability and biases of real-world clinical data.
- Logistic regression is limited by linear assumptions and may fail to capture complex interactions.
- Random forests and neural networks are harder to interpret and can overfit if not properly regularized.
- SMOTE may generate borderline or unrealistic samples, distorting learning if used without caution.

#### Future Work and Potential Improvements

Several avenues could further improve model performance and generalizability:

- **Bayesian Optimization for hyperparameters**: Replacing random or grid search with Bayesian Optimization could lead to more efficient and principled tuning of model weights, especially in neural networks or ensemble methods.

- **Explore SMOTETomek for resampling**: This technique combines SMOTE with Tomek Links to reduce class overlap while preserving minority class examples. It offers a more conservative alternative to ENN, potentially improving performance on noisy datasets.

- **Test additional models**: Gradient boosting frameworks such as XGBoost or LightGBM may improve performance while allowing post-hoc interpretability through SHAP.

- **Enforce domain-informed constraints**: Adding structural priors or biologically informed rules to neural networks may improve learning stability and generalization, particularly in low-data regimes.

"""